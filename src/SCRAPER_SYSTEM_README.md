# ๐ค ูุธุงู ุฌูุจ ุงููุธุงุฆู ุงูุชููุงุฆู - Job Scraper System

## ๐ ูุธุฑุฉ ุนุงูุฉ

ูุธุงู ูุชูุฏู ูุฌูุจ ุงููุธุงุฆู ุชููุงุฆูุงู ูู ููุงูุน ุฎุงุฑุฌูุฉ ูุฅุถุงูุชูุง ุฅูู ููุตุฉ ุนููุงู ูููุธุงุฆู ุฏูู ุฃู ุชุฏุฎู ูุฏูู.

### โจ ุงููููุฒุงุช ุงูุฑุฆูุณูุฉ

- โ **ุฌูุจ ุชููุงุฆู**: ุงุณุชูุฑุงุฏ ุงููุธุงุฆู ูู ููุงูุน ุฎุงุฑุฌูุฉ ุจุดูู ุชููุงุฆู
- โ **ุชุฌูุจ ุงูุชูุฑุงุฑ**: ูุญุต ุฐูู ูููุธุงุฆู ุงูููุฑุฑุฉ
- โ **ูุงุฌูุฉ ุฅุฏุงุฑูุฉ**: ุตูุญุฉ admin ุณููุฉ ุงูุงุณุชุฎุฏุงู
- โ **ุฌุฏููุฉ ูุฑูุฉ**: ุฏุนู ุนุฏุฉ ุทุฑู ููุฌุฏููุฉ ุงูุชููุงุฆูุฉ
- โ **ุฅุญุตุงุฆูุงุช ููุตูุฉ**: ุชูุงุฑูุฑ ุนู ูู ุนูููุฉ ุฌูุจ
- โ **ูุงุจู ููุชูุณุน**: ุณูููุฉ ุฅุถุงูุฉ ููุงูุน ุฌุฏูุฏุฉ

---

## ๐๏ธ ุงูุจููุฉ ุงูุชูููุฉ

### Backend Components

```
/supabase/functions/server/
โโโ index.tsx                 # Main server with /admin/scrape-jobs endpoint
โโโ job-scraper.tsx          # Web scraping logic
```

### Frontend Components

```
/pages/admin/
โโโ AdminScraperPage.tsx     # Admin UI for manual scraping

/utils/
โโโ adminApi.ts              # API functions (scrapeJobs)
```

### Database

```sql
-- jobs table (existing)
- id (uuid)
- title (text)
- description (text)
- application_url (text)
- date (date)
- created_at (timestamp)
```

---

## ๐ ููููุฉ ุงูุงุณุชุฎุฏุงู

### 1๏ธโฃ ุงูุชุดุบูู ุงููุฏูู

**ุงูุฎุทูุงุช:**
1. ุชุณุฌูู ุงูุฏุฎูู ูู Admin
2. ุงูุฐูุงุจ ุฅูู ููุญุฉ ุงูุชุญูู โ "ุฌูุจ ุงููุธุงุฆู ุชููุงุฆูุงู"
3. ุงูุถุบุท ุนูู ุฒุฑ "ุฌูุจ ุงููุธุงุฆู ุงูุขู"
4. ุงูุชุธุงุฑ ุงูุชูุงู ุงูุนูููุฉ (15-60 ุซุงููุฉ ุนุงุฏุฉ)
5. ูุฑุงุฌุนุฉ ุงููุชุงุฆุฌ ูุงูุฅุญุตุงุฆูุงุช

**ุงููุชูุฌุฉ ุงููุชููุนุฉ:**
```
ุงููุธุงุฆู ุงููุณุชุฎุฑุฌุฉ: 45
ุงููุธุงุฆู ุงููุถุงูุฉ: 12
ุงููุธุงุฆู ุงูููุฑุฑุฉ: 33
```

---

### 2๏ธโฃ ุงูุฌุฏููุฉ ุงูุชููุงุฆูุฉ

#### ุฎูุงุฑ A: Cron-Job.org (ููุตู ุจู ูููุจุชุฏุฆูู)

**ุงููููุฒุงุช:**
- ๐ ูุฌุงูู
- ๐ง ุณูู ุงูุฅุนุฏุงุฏ
- ๐ ููุญุฉ ุชุญูู ุจุณูุทุฉ
- ๐ง ุฅุดุนุงุฑุงุช ุจุงูุจุฑูุฏ ุนูุฏ ุงููุดู

**ุงูุฎุทูุงุช:**

1. **ุงูุชุณุฌูู**
   - ุงูุฐูุงุจ ุฅูู https://cron-job.org
   - ุฅูุดุงุก ุญุณุงุจ ูุฌุงูู

2. **ุฅูุดุงุก Cron Job**
   - ุงูุถุบุท ุนูู "Create Cronjob"
   - Title: `Scrape Jobs - Oman Jobs Platform`

3. **ุงูุฅุนุฏุงุฏุงุช ุงูุชูุตูููุฉ**

   **URL:**
   ```
   https://YOUR_PROJECT_ID.supabase.co/functions/v1/make-server-8a20c00b/admin/scrape-jobs
   ```

   **Request Method:** `POST`

   **Headers:**
   ```
   Content-Type: application/json
   X-Admin-Token: YOUR_ADMIN_TOKEN
   ```

   **Request Body (ุงุฎุชูุงุฑู):**
   ```json
   {
     "sourceUrl": "https://jobsofoman.com/ar/index.php"
   }
   ```

4. **ุฌุฏููุฉ ุงูููุช**
   
   ุงุฎุชุฑ ุฃุญุฏ ุงูุฎูุงุฑุงุช:
   - **ููููุงู ุงูุณุงุนุฉ 2 ุตุจุงุญุงู**: `0 2 * * *`
   - **ูู 6 ุณุงุนุงุช**: `0 */6 * * *`
   - **ูุฑุชูู ููููุงู (8 ุตุจุงุญุงู ู 8 ูุณุงุกู)**: `0 8,20 * * *`
   - **ูู 12 ุณุงุนุฉ**: `0 */12 * * *`

5. **ุญูุธ ูุชูุนูู**
   - ูุฑุงุฌุนุฉ ุงูุฅุนุฏุงุฏุงุช
   - ุงูุถุบุท ุนูู "Create Cronjob"
   - ุงูุชุฃูุฏ ูู ุชูุนูู ุงูู cronjob

---

#### ุฎูุงุฑ B: GitHub Actions (ููุตู ุจู ูููุญุชุฑููู)

**ุงููููุฒุงุช:**
- ๐ ูุฌุงูู ุชูุงูุงู
- ๐ ุฃูุซุฑ ุฃูุงูุงู (Secrets management)
- ๐ Version control
- ๐ ุณูููุฉ ุงูุชุญุฏูุซ

**ุงูุฎุทูุงุช:**

1. **ุฅูุดุงุก Repository**
   ```bash
   mkdir oman-jobs-automation
   cd oman-jobs-automation
   git init
   ```

2. **ุฅูุดุงุก Workflow File**
   
   ุฃูุดุฆ ููู: `.github/workflows/scrape-jobs.yml`

   ```yaml
   name: Auto Scrape Jobs
   
   on:
     schedule:
       # ูุนูู ูู 6 ุณุงุนุงุช
       - cron: '0 */6 * * *'
     
     # ูููู ุชุดุบููู ูุฏููุงู
     workflow_dispatch:
   
   jobs:
     scrape-jobs:
       runs-on: ubuntu-latest
       
       steps:
         - name: Scrape Jobs from JobsOfOman
           run: |
             response=$(curl -X POST \
               https://${{ secrets.SUPABASE_PROJECT_ID }}.supabase.co/functions/v1/make-server-8a20c00b/admin/scrape-jobs \
               -H "Content-Type: application/json" \
               -H "Authorization: Bearer ${{ secrets.SUPABASE_ANON_KEY }}" \
               -H "X-Admin-Token: ${{ secrets.ADMIN_TOKEN }}" \
               -d '{"sourceUrl": "https://jobsofoman.com/ar/index.php"}' \
               -w "\n%{http_code}")
             
             echo "$response"
             http_code=$(echo "$response" | tail -n1)
             
             if [ "$http_code" -ne 200 ]; then
               echo "Error: HTTP $http_code"
               exit 1
             fi
         
         - name: Log Success
           if: success()
           run: echo "โ Jobs scraped successfully at $(date)"
         
         - name: Log Failure
           if: failure()
           run: echo "โ Job scraping failed at $(date)"
   ```

3. **ุฅุถุงูุฉ GitHub Secrets**
   
   ูู Repository Settings โ Secrets โ Actions:
   
   - `SUPABASE_PROJECT_ID`: ูุนุฑู ูุดุฑูุน Supabase
   - `SUPABASE_ANON_KEY`: ุงูููุชุงุญ ุงูุนุงู ูู Supabase
   - `ADMIN_TOKEN`: admin token ูู ุชุณุฌูู ุงูุฏุฎูู

4. **Push ุฅูู GitHub**
   ```bash
   git add .
   git commit -m "Add job scraping automation"
   git push origin main
   ```

5. **ุชูุนูู Actions**
   - ุงูุฐูุงุจ ุฅูู Actions tab
   - ุชูุนูู workflows
   - (ุงุฎุชูุงุฑู) ุชุดุบูู ูุฏูู ููุงุฎุชุจุงุฑ: "Run workflow"

---

#### ุฎูุงุฑ C: Vercel Cron Jobs

**ููููุงูุน ุงููุณุชุถุงูุฉ ุนูู Vercel:**

1. ุฅูุดุงุก ููู `vercel.json`:
   ```json
   {
     "crons": [{
       "path": "/api/scrape-jobs",
       "schedule": "0 */6 * * *"
     }]
   }
   ```

2. ุฅูุดุงุก API route ูู `/pages/api/scrape-jobs.ts`:
   ```typescript
   export default async function handler(req, res) {
     const response = await fetch(
       'https://YOUR_PROJECT.supabase.co/functions/v1/make-server-8a20c00b/admin/scrape-jobs',
       {
         method: 'POST',
         headers: {
           'Content-Type': 'application/json',
           'X-Admin-Token': process.env.ADMIN_TOKEN
         }
       }
     );
     const data = await response.json();
     res.json(data);
   }
   ```

---

## ๐ง ุงูุชูููู ุงููุชูุฏู

### ุชุฎุตูุต Scraper

**ุฅุถุงูุฉ ูููุน ุฌุฏูุฏ:**

ูู `/supabase/functions/server/job-scraper.tsx`:

```typescript
export async function scrapeNewSite(): Promise<ScrapedJob[]> {
  const url = 'https://newsite.com/jobs';
  const jobs: ScrapedJob[] = [];
  
  try {
    const response = await fetch(url, {
      headers: {
        'User-Agent': 'Mozilla/5.0...'
      }
    });
    
    const html = await response.text();
    const doc = new DOMParser().parseFromString(html, 'text/html');
    
    // ุงุณุชุฎุฑุงุฌ ุงููุธุงุฆู ุญุณุจ ุจููุฉ ุงููููุน
    const jobElements = doc.querySelectorAll('.job-item');
    
    for (const el of jobElements) {
      jobs.push({
        title: el.querySelector('.title')?.textContent || '',
        description: el.querySelector('.desc')?.textContent || '',
        applicationUrl: el.querySelector('a')?.href || '',
        date: new Date().toISOString().split('T')[0],
        source: 'newsite.com'
      });
    }
    
    return jobs;
  } catch (error) {
    console.error('Error:', error);
    throw error;
  }
}
```

**ุชุญุฏูุซ `scrapeWebsite` function:**

```typescript
export async function scrapeWebsite(url: string): Promise<ScrapedJob[]> {
  if (url.includes('jobsofoman.com')) {
    return scrapeJobsOfOman();
  } else if (url.includes('newsite.com')) {
    return scrapeNewSite();
  }
  
  throw new Error(`Unsupported URL: ${url}`);
}
```

---

## ๐ ุงููุฑุงูุจุฉ ูุงูุชุญููู

### ูุคุดุฑุงุช ุงูุฃุฏุงุก (KPIs)

ุฑุงูุจ ูุฐู ุงููุคุดุฑุงุช:

- **Success Rate**: ูุณุจุฉ ุงูุนูููุงุช ุงููุงุฌุญุฉ
- **Jobs Scraped**: ุฅุฌูุงูู ุงููุธุงุฆู ุงููุณุชุฎุฑุฌุฉ
- **Jobs Added**: ุงููุธุงุฆู ุงูุฌุฏูุฏุฉ ุงููุถุงูุฉ
- **Duplicate Rate**: ูุณุจุฉ ุงููุธุงุฆู ุงูููุฑุฑุฉ
- **Scraping Duration**: ููุช ูู ุนูููุฉ

### Logging

**ูู ุงูุณูุฑูุฑ:**
```typescript
console.log(`Starting scrape at ${new Date().toISOString()}`);
console.log(`Found ${jobs.length} jobs`);
console.log(`Added ${newJobs.length} new jobs`);
```

**ูู Frontend:**
```typescript
console.log('Scrape result:', result);
```

---

## ๐ ุญู ุงููุดุงูู

### ุงููุดุงูู ุงูุดุงุฆุนุฉ

| ุงููุดููุฉ | ุงูุณุจุจ ุงููุญุชูู | ุงูุญู |
|---------|---------------|------|
| No jobs found | ุชุบููุฑ ูู ุจููุฉ HTML | ุชุญุฏูุซ selectors ูู job-scraper.tsx |
| All duplicates | ูุง ูุธุงุฆู ุฌุฏูุฏุฉ | ุทุจูุนูุ ุงูุชุธุฑ ููุช ุฃุทูู ุจูู ุงูุนูููุงุช |
| Timeout error | ุงููููุน ุจุทูุก | ุฒูุงุฏุฉ timeout ุฃู ุฅุนุงุฏุฉ ุงููุญุงููุฉ |
| 401 Unauthorized | Admin token ุฎุงุทุฆ | ุงูุชุญูู ูู token |
| 500 Server Error | ูุดููุฉ ูู ุงูุณูุฑูุฑ | ูุญุต console logs |

### Debug Mode

ูุชูุนูู debug modeุ ุฃุถู ูู `job-scraper.tsx`:

```typescript
const DEBUG = true;

if (DEBUG) {
  console.log('HTML length:', html.length);
  console.log('Job elements found:', jobElements.length);
  console.log('First job:', jobs[0]);
}
```

---

## ๐ ุงูุฃูุงู

### Best Practices

1. **Tokens**:
   - โ ุงุณุชุฎุฏู environment variables
   - โ ูุง ุชุดุงุฑู tokens ูู ุงูููุฏ
   - โ ุบููุฑ tokens ุฏูุฑูุงู

2. **Rate Limiting**:
   - โ ูุง ุชุฌุฏูู ุฃูุซุฑ ูู ูู 6 ุณุงุนุงุช
   - โ ุงุญุชุฑู robots.txt
   - โ ุงุณุชุฎุฏู User-Agent ูุงูุนู

3. **Data Validation**:
   - โ ุชุญูู ูู ุทูู ุงูุนููุงู ูุงููุตู
   - โ ุชูุธูู HTML tags
   - โ validate URLs

---

## ๐ ุงูุชุญุณููุงุช ุงููุณุชูุจููุฉ

### Roadmap

- [ ] ุฏุนู ููุงูุน ุฅุถุงููุฉ
- [ ] ุชุญุณูู ุงุณุชุฎุฑุงุฌ ุงูุจูุงูุงุช ุจู AI
- [ ] ุฅุดุนุงุฑุงุช ุนูุฏ ุฅุถุงูุฉ ูุธุงุฆู ุฌุฏูุฏุฉ
- [ ] ุชุตููู ุชููุงุฆู ูููุธุงุฆู
- [ ] API endpoint ุนุงู ูููุณุชุฎุฏููู
- [ ] Dashboard ููุฅุญุตุงุฆูุงุช ุงูุชุงุฑูุฎูุฉ

---

## ๐ ุงูููุงุฑุฏ

### ูููุงุช ูููุฉ

- `JOB_SCRAPER_GUIDE.md`: ุฏููู ุดุงูู ููุตู
- `QUICK_START_SCRAPER.md`: ุฏููู ุงูุจุฏุก ุงูุณุฑูุน
- `/supabase/functions/server/job-scraper.tsx`: ููุฏ ุงูู scraper
- `/pages/admin/AdminScraperPage.tsx`: ูุงุฌูุฉ Admin

### ุฑูุงุจุท ูููุฏุฉ

- [Supabase Edge Functions Docs](https://supabase.com/docs/guides/functions)
- [Deno DOM Parser](https://deno.land/x/deno_dom)
- [Cron Expression Generator](https://crontab.guru/)
- [GitHub Actions Docs](https://docs.github.com/en/actions)

---

## ๐ก ูุตุงุฆุญ ุงูุงุณุชุฎุฏุงู

1. **ุงุจุฏุฃ ุจุงูุงุฎุชุจุงุฑ ุงููุฏูู** ูุจู ุงูุฌุฏููุฉ ุงูุชููุงุฆูุฉ
2. **ุฑุงุฌุน ุงููุชุงุฆุฌ** ุจุนุฏ ุฃูู ุนูููุฉ ุฌูุจ
3. **ุงุญุฐู ุงููุธุงุฆู ุบูุฑ ุงูููุงุณุจุฉ** ูุฏููุงู
4. **ุฌุฏูู ุงูุนูููุฉ ูู ุฃููุงุช ูุงุฏุฆุฉ** (ููุชุตู ุงูููู)
5. **ุฑุงูุจ ุงูุฅุญุตุงุฆูุงุช** ุฃุณุจูุนูุงู

---

## ๐ ุงูุฏุนู

ูููุณุงุนุฏุฉ:
1. ุฑุงุฌุน ูููุงุช ุงูุชูุซูู
2. ุงูุญุต console logs
3. ุชุญูู ูู ุฅุนุฏุงุฏุงุช Supabase
4. ุงุชุตู ุจูุทูุฑ ุงููุธุงู

---

## ๐ ุงูุชุฑุฎูุต

ูุฐุง ุงููุธุงู ุฌุฒุก ูู ููุตุฉ ุนููุงู ูููุธุงุฆู ูุฎุงุถุน ูููุณ ุงูุชุฑุฎูุต.

---

**ุขุฎุฑ ุชุญุฏูุซ:** ุฏูุณูุจุฑ 2024  
**ุงูุฅุตุฏุงุฑ:** 1.0.0  
**ุงูุญุงูุฉ:** โ ุฌุงูุฒ ููุงุณุชุฎุฏุงู

---

## โ Checklist ููุฅุนุฏุงุฏ ุงูุฃููู

- [ ] ุงุฎุชุจุงุฑ ุงูุชุดุบูู ุงููุฏูู
- [ ] ุงูุชุญูู ูู ูุชุงุฆุฌ ุฃูู ุนูููุฉ ุฌูุจ
- [ ] ูุฑุงุฌุนุฉ ุฌูุฏุฉ ุงูุจูุงูุงุช ุงููุณุชุฎุฑุฌุฉ
- [ ] ุฅุนุฏุงุฏ ุงูุฌุฏููุฉ ุงูุชููุงุฆูุฉ
- [ ] ุฅุถุงูุฉ monitoring/alerts
- [ ] ุชูุซูู ุฃู ุชุฎุตูุตุงุช

๐ **ูุจุฑูู! ูุธุงู ุฌูุจ ุงููุธุงุฆู ุงูุชููุงุฆู ุฌุงูุฒ ููุนูู!**
